#!/usr/bin/env python3
"""
Calculate ELO ratings from tournament results.

Parses the CSV battle logs generated by the tournament script and computes
ELO ratings for all participants using standard ELO algorithm.

Usage:
    python scripts/calculate_elo.py \
        --tournament_dir ~/gen1_tournament_results \
        --k_factor 32 \
        --initial_rating 1500

Output:
    - final_elo_ratings.json: ELO ratings for all models
    - elo_progression.csv: ELO changes over time
    - matchup_matrix.csv: Win/loss record for each pairing
"""

import os
import sys
import json
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple

# Add metamon to path
sys.path.insert(0, str(Path(__file__).parent.parent))


def expected_score(rating_a: float, rating_b: float) -> float:
    """Calculate expected score for player A against player B."""
    return 1.0 / (1.0 + 10 ** ((rating_b - rating_a) / 400.0))


def update_elo(
    rating: float, expected: float, actual: float, k_factor: float = 32.0
) -> float:
    """Update ELO rating based on game result."""
    return rating + k_factor * (actual - expected)


def parse_battle_logs(tournament_dir: str) -> pd.DataFrame:
    """
    Parse all battle_log CSV files from tournament directory.

    Returns DataFrame with columns:
    - Player Username
    - Team File
    - Opponent Username
    - Result (WIN/LOSS)
    - Turn Count
    - Battle ID
    """
    team_results_dir = os.path.join(tournament_dir, "team_results")

    if not os.path.exists(team_results_dir):
        raise FileNotFoundError(
            f"Team results directory not found: {team_results_dir}"
        )

    # Find all CSV files
    csv_files = list(Path(team_results_dir).glob("battle_log_*.csv"))

    if not csv_files:
        raise FileNotFoundError(
            f"No battle log CSV files found in {team_results_dir}"
        )

    print(f"Found {len(csv_files)} battle log files")

    # Read and concatenate all CSVs
    dfs = []
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file, skipinitialspace=True)
            dfs.append(df)
        except Exception as e:
            print(f"Warning: Could not read {csv_file}: {e}")

    if not dfs:
        raise ValueError("No valid CSV files could be parsed")

    all_battles = pd.concat(dfs, ignore_index=True)

    print(f"Loaded {len(all_battles)} total battles")

    return all_battles


def extract_model_name(username: str) -> str:
    """
    Extract model name from username.

    Usernames are formatted as: <ModelName>_<timestamp>_<A/B>
    Example: "Gen1Binary_143052_A" -> "Gen1Binary"
    """
    # Remove timestamp and A/B suffix
    parts = username.split("_")
    if len(parts) >= 2:
        # Take everything except last 2 parts (timestamp and A/B)
        return "_".join(parts[:-2])
    return username


def calculate_elo_ratings(
    battles_df: pd.DataFrame,
    initial_rating: float = 1500.0,
    k_factor: float = 32.0,
) -> Tuple[Dict[str, float], List[Dict]]:
    """
    Calculate ELO ratings from battle results.

    Returns:
        - Final ELO ratings dict
        - ELO progression list (for plotting)
    """

    # Initialize ratings
    ratings = defaultdict(lambda: initial_rating)

    # Track progression
    progression = []

    # Process battles in order
    for idx, row in battles_df.iterrows():
        player = extract_model_name(row["Player Username"])
        opponent = extract_model_name(row["Opponent Username"])
        result = row["Result"]

        # Get current ratings
        player_rating = ratings[player]
        opponent_rating = ratings[opponent]

        # Calculate expected scores
        player_expected = expected_score(player_rating, opponent_rating)

        # Actual score (1 for win, 0 for loss)
        player_actual = 1.0 if result == "WIN" else 0.0

        # Update ratings
        new_player_rating = update_elo(
            player_rating, player_expected, player_actual, k_factor
        )
        new_opponent_rating = update_elo(
            opponent_rating, 1 - player_expected, 1 - player_actual, k_factor
        )

        ratings[player] = new_player_rating
        ratings[opponent] = new_opponent_rating

        # Record progression
        progression.append(
            {
                "battle_num": idx,
                "player": player,
                "opponent": opponent,
                "result": result,
                "player_rating_before": player_rating,
                "player_rating_after": new_player_rating,
                "opponent_rating_before": opponent_rating,
                "opponent_rating_after": new_opponent_rating,
            }
        )

    return dict(ratings), progression


def create_matchup_matrix(battles_df: pd.DataFrame) -> pd.DataFrame:
    """
    Create win/loss matrix showing head-to-head records.

    Returns DataFrame where matrix[i][j] = "W-L" (wins-losses for model i vs model j)
    """

    # Extract model names
    battles_df = battles_df.copy()
    battles_df["Player_Model"] = battles_df["Player Username"].apply(extract_model_name)
    battles_df["Opponent_Model"] = battles_df["Opponent Username"].apply(
        extract_model_name
    )

    # Get unique models
    models = sorted(
        set(battles_df["Player_Model"].unique())
        | set(battles_df["Opponent_Model"].unique())
    )

    # Initialize matrix
    matrix = pd.DataFrame(index=models, columns=models, dtype=str)

    # Fill matrix
    for model1 in models:
        for model2 in models:
            if model1 == model2:
                matrix.loc[model1, model2] = "-"
                continue

            # Count wins and losses
            matchup = battles_df[
                (battles_df["Player_Model"] == model1)
                & (battles_df["Opponent_Model"] == model2)
            ]

            wins = (matchup["Result"] == "WIN").sum()
            losses = (matchup["Result"] == "LOSS").sum()

            if wins + losses > 0:
                matrix.loc[model1, model2] = f"{wins}-{losses}"
            else:
                matrix.loc[model1, model2] = "0-0"

    return matrix


def calculate_statistics(battles_df: pd.DataFrame) -> pd.DataFrame:
    """Calculate detailed statistics for each model."""

    battles_df = battles_df.copy()
    battles_df["Player_Model"] = battles_df["Player Username"].apply(extract_model_name)

    stats = []

    for model in battles_df["Player_Model"].unique():
        model_battles = battles_df[battles_df["Player_Model"] == model]

        wins = (model_battles["Result"] == "WIN").sum()
        losses = (model_battles["Result"] == "LOSS").sum()
        total = wins + losses

        win_rate = wins / total if total > 0 else 0
        avg_turns = model_battles["Turn Count"].mean()

        stats.append(
            {
                "Model": model,
                "Wins": wins,
                "Losses": losses,
                "Total Battles": total,
                "Win Rate": f"{win_rate:.1%}",
                "Avg Turn Count": f"{avg_turns:.1f}",
            }
        )

    return pd.DataFrame(stats).sort_values("Wins", ascending=False)


def main():
    parser = argparse.ArgumentParser(
        description="Calculate ELO ratings from tournament results"
    )
    parser.add_argument(
        "--tournament_dir",
        type=str,
        required=True,
        help="Directory containing tournament results (with team_results/ subdirectory)",
    )
    parser.add_argument(
        "--k_factor",
        type=float,
        default=32.0,
        help="ELO K-factor (higher = more volatile ratings)",
    )
    parser.add_argument(
        "--initial_rating",
        type=float,
        default=1500.0,
        help="Starting ELO rating for all models",
    )
    parser.add_argument(
        "--output_prefix",
        type=str,
        default="elo_results",
        help="Prefix for output files",
    )

    args = parser.parse_args()

    tournament_dir = os.path.expanduser(args.tournament_dir)

    print(f"\n{'='*80}")
    print(f"ELO RATING CALCULATION")
    print(f"Tournament directory: {tournament_dir}")
    print(f"K-factor: {args.k_factor}")
    print(f"Initial rating: {args.initial_rating}")
    print(f"{'='*80}\n")

    # Parse battle logs
    print("Parsing battle logs...")
    battles_df = parse_battle_logs(tournament_dir)

    # Calculate ELO ratings
    print("Calculating ELO ratings...")
    final_ratings, progression = calculate_elo_ratings(
        battles_df, args.initial_rating, args.k_factor
    )

    # Sort by rating
    sorted_ratings = sorted(final_ratings.items(), key=lambda x: x[1], reverse=True)

    # Print rankings
    print(f"\n{'='*80}")
    print(f"FINAL ELO RANKINGS")
    print(f"{'='*80}")
    print(f"{'Rank':<6} {'Model':<30} {'ELO Rating':<12} {'Change':<10}")
    print(f"{'-'*80}")

    for rank, (model, rating) in enumerate(sorted_ratings, 1):
        change = rating - args.initial_rating
        change_str = f"{change:+.1f}"
        print(f"{rank:<6} {model:<30} {rating:<12.1f} {change_str:<10}")

    # Create matchup matrix
    print(f"\n\nCreating matchup matrix...")
    matchup_matrix = create_matchup_matrix(battles_df)

    # Calculate statistics
    print(f"Calculating statistics...")
    stats_df = calculate_statistics(battles_df)

    # Save results
    output_dir = tournament_dir
    os.makedirs(output_dir, exist_ok=True)

    # Save ELO ratings (JSON)
    ratings_file = os.path.join(output_dir, f"{args.output_prefix}_ratings.json")
    with open(ratings_file, "w") as f:
        json.dump(
            {
                "k_factor": args.k_factor,
                "initial_rating": args.initial_rating,
                "ratings": {
                    model: {"elo": rating, "rank": rank}
                    for rank, (model, rating) in enumerate(sorted_ratings, 1)
                },
            },
            f,
            indent=2,
        )
    print(f"✓ Saved ELO ratings: {ratings_file}")

    # Save progression (CSV)
    progression_file = os.path.join(output_dir, f"{args.output_prefix}_progression.csv")
    pd.DataFrame(progression).to_csv(progression_file, index=False)
    print(f"✓ Saved ELO progression: {progression_file}")

    # Save matchup matrix (CSV)
    matrix_file = os.path.join(output_dir, f"{args.output_prefix}_matchup_matrix.csv")
    matchup_matrix.to_csv(matrix_file)
    print(f"✓ Saved matchup matrix: {matrix_file}")

    # Save statistics (CSV)
    stats_file = os.path.join(output_dir, f"{args.output_prefix}_statistics.csv")
    stats_df.to_csv(stats_file, index=False)
    print(f"✓ Saved statistics: {stats_file}")

    print(f"\n{'='*80}")
    print(f"ELO CALCULATION COMPLETE")
    print(f"{'='*80}\n")

    # Print statistics
    print(stats_df.to_string(index=False))

    return 0


if __name__ == "__main__":
    sys.exit(main())
