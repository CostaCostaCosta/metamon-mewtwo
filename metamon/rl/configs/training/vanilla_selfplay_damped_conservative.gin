# Conservative Dynamic Damping for Pretrained Policy Finetuning
#
# This configuration prevents catastrophic drift when finetuning a strong pretrained
# Pokémon policy (e.g., SyntheticRLV2). The priority is STABILITY, not speed.
#
# Key design principles:
#   1. Learning rates reduced 10× from baseline (very small PPO steps)
#   2. Strict KL target with tight tolerance (0.01 ± 25%)
#   3. Strong baseline KL damping with NO DECAY (alpha=0.0)
#   4. Gentle controller adjustments to avoid overcorrection
#   5. Bias toward on-policy data for stable advantage estimates
#
# This config addresses the failure mode where:
#   - Large early updates destroyed the pretrained policy
#   - KL coefficient only decayed and never fought back
#   - Policy collapsed before critic could calibrate
#
# Expected behavior:
#   - Very slow, stable updates throughout training
#   - KL divergence stays near 0.01 (never drifts far)
#   - Damping coefficient holds steady or increases (never weakens)
#   - Policy performance improves gradually without collapse

import amago.agent

# Reward shaping
agent.Agent.reward_multiplier = 10.
agent.MultiTaskAgent.reward_multiplier = 10.

# Target network update rate
agent.Agent.tau = .004
agent.MultiTaskAgent.tau = .004

# Number of actions sampled for value estimation
agent.Agent.num_actions_for_value_in_critic_loss = 1
agent.MultiTaskAgent.num_actions_for_value_in_critic_loss = 3

agent.Agent.num_actions_for_value_in_actor_loss = 1
agent.MultiTaskAgent.num_actions_for_value_in_actor_loss = 3

# =========================
# Data Mix: Bias Toward On-Policy
# =========================
# Temporarily raise online/self-play weighting for early stability.
# Without advantage filtering, offline data produces noisier advantages
# that can destabilize PPO. This ensures the agent learns primarily from
# coherent on-policy trajectories until damping stabilizes.
agent.Agent.online_coeff = 0.75
agent.MultiTaskAgent.online_coeff = 0.75
agent.Agent.offline_coeff = 0.25
agent.MultiTaskAgent.offline_coeff = 0.25
agent.Agent.fbc_filter_func = @agent.binary_filter
agent.MultiTaskAgent.fbc_filter_func = @agent.binary_filter

# =========================
# Core Optimizer Settings
# =========================
MetamonAMAGOExperiment.l2_coeff = 5e-5

# STRONGLY REDUCED learning rates (~10× smaller)
# Pretrained policies require much smaller PPO steps to avoid destructive updates
MetamonAMAGOExperiment.learning_rate = 1e-5      # Base LR (10× reduction)
MetamonAMAGOExperiment.lr_warmup_steps = 500     # Short warmup
MetamonAMAGOExperiment.max_lr = 2e-5             # Never go higher
MetamonAMAGOExperiment.min_lr = 1e-6             # Conservative floor

MetamonAMAGOExperiment.grad_clip = 0.3           # Tight gradient clipping

MetamonAMAGOExperiment.critic_loss_weight = 10.

# =========================
# Dynamic Damping (KL): STRICT + NO DECAY
# =========================
MetamonAMAGOExperiment.use_dynamic_damping = True

# STRICT KL target with TIGHT tolerance
# Keep policy updates very small and safe until critic calibrates
MetamonAMAGOExperiment.target_kl_per_step = 0.01  # Conservative target
MetamonAMAGOExperiment.kl_tolerance = 1.25        # Narrow acceptable band [0.008, 0.0125]

# STRONG baseline KL coefficient + NO DECAY
# In failed runs, KL coefficient only decayed and never fought back.
# This setup ensures damping NEVER weakens unless controller explicitly relaxes it.
MetamonAMAGOExperiment.kl_coef_init = 0.30        # Strong initial damping
MetamonAMAGOExperiment.kl_coef_max = 0.60         # High ceiling for controller

# POWER-LAW SCHEDULE DISABLED
# No automatic decay - damping stays protective throughout training
MetamonAMAGOExperiment.kl_power_alpha = 0.0       # NO DECAY
MetamonAMAGOExperiment.kl_schedule_steps = 1_000_000  # (unused)

# SOFTER controller adjustments (on top of steady baseline)
# Allow controller to correct, but gently to avoid overcorrection
MetamonAMAGOExperiment.lr_shrink_factor = 0.5     # Halve LR when KL too high
MetamonAMAGOExperiment.lr_grow_factor = 1.05      # Grow very slowly when KL low

MetamonAMAGOExperiment.kl_coef_growth_factor = 1.5    # Gentle increase if KL high
MetamonAMAGOExperiment.kl_coef_decay_factor = 0.95    # Gentle decrease if KL low

# =========================
# Dynamic Damping (Entropy): Keep as in v2
# =========================
# Entropy behavior looked reasonable in previous runs - instability was from KL, not entropy
MetamonAMAGOExperiment.ent_coef_init = 0.02       # Same as v2
MetamonAMAGOExperiment.ent_coef_min = 0.003       # Floor lowered slightly

# Gentle entropy decay (keep v2 schedule)
MetamonAMAGOExperiment.ent_power_alpha = 0.3      # Same as v2
MetamonAMAGOExperiment.ent_schedule_steps = 1_000_000

# =========================
# Expected Behavior
# =========================
# During training, you should observe:
#   1. damping/kl_coef: Starts at 0.30, NEVER decays automatically (only controller adjusts)
#   2. damping/ent_coef: Starts at 0.02, decays gently toward 0.003
#   3. KL Divergence: Stays tightly around 0.01 (rarely exceeds 0.0125)
#   4. damping/lr: Adjusts reactively (shrinks if KL spikes, grows slowly if stable)
#   5. Policy Entropy: Trends downward gradually but stays healthy (> 1.0)
#
# Success indicators:
#   - No catastrophic KL spikes (>> 0.02)
#   - Policy performance improves slowly and steadily
#   - Damping coefficient increases when needed (fights instability)
#   - Critic loss stabilizes without policy collapse
#
# Red flags:
#   - KL >> 0.015 persistently: Increase kl_coef_init or decrease learning_rate
#   - Entropy < 0.5: Policy collapsed despite damping - restart with higher kl_coef_init
#   - KL ~ 0.0: Too conservative - can increase target_kl_per_step to 0.015
#   - LR keeps shrinking to min_lr: Persistent instability - reduce lr_grow_factor further
