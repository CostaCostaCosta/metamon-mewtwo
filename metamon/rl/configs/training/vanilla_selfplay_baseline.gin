# Vanilla Self-Play Training Config (Baseline - No Damping)
#
# This config is for vanilla self-play experiments without dynamic damping.
# Use this as a baseline to compare against dynamic damping performance.

import amago.agent

# Reward shaping
agent.Agent.reward_multiplier = 10.
agent.MultiTaskAgent.reward_multiplier = 10.

# Target network update rate
agent.Agent.tau = .004
agent.MultiTaskAgent.tau = .004

# Number of actions sampled for value estimation
agent.Agent.num_actions_for_value_in_critic_loss = 1
agent.MultiTaskAgent.num_actions_for_value_in_critic_loss = 3

agent.Agent.num_actions_for_value_in_actor_loss = 1
agent.MultiTaskAgent.num_actions_for_value_in_actor_loss = 3

# Offline RL coefficients for self-play
# Use balanced mix of online and offline for self-play
agent.Agent.online_coeff = 0.5
agent.MultiTaskAgent.online_coeff = 0.5
agent.Agent.offline_coeff = 0.5
agent.MultiTaskAgent.offline_coeff = 0.5
agent.Agent.fbc_filter_func = @agent.binary_filter
agent.MultiTaskAgent.fbc_filter_func = @agent.binary_filter

# Training hyperparameters
MetamonAMAGOExperiment.l2_coeff = 5e-5  # Slightly lower than offline-only
MetamonAMAGOExperiment.learning_rate = 1e-4  # Lower LR for self-play stability
MetamonAMAGOExperiment.grad_clip = 1.5
MetamonAMAGOExperiment.critic_loss_weight = 10.
MetamonAMAGOExperiment.lr_warmup_steps = 500  # Shorter warmup for fine-tuning

# Dynamic damping: DISABLED for baseline
MetamonAMAGOExperiment.use_dynamic_damping = False
