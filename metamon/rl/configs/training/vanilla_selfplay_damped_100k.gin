# Dynamic Damping V2 - Optimized for 100k Sample First Run
#
# This config adapts the v2 schedule-based damping for a smaller initial run.
# Key changes from vanilla_selfplay_damped_schedule.gin:
#   - Schedule steps scaled down from 1M to 50k (matches 100k sample budget)
#   - Slightly more aggressive power-law decay (meaningful change over run)
#   - Target KL lowered to 0.01 (matches Ataraxos conservative updates)
#   - Tolerance widened to 2.0 (allow more variance in early training)
#
# Expected behavior over 100k samples:
#   - Early (0-20k): Strong damping prevents collapse, high entropy exploration
#   - Mid (20-60k): Gradual relaxation, controller finds equilibrium
#   - Late (60-100k): Lower entropy, more exploitation, stable KL
#
# Use this for your FIRST self-play run to validate the approach, then scale up.

import amago.agent

# Reward shaping (keep same as v2)
agent.Agent.reward_multiplier = 10.
agent.MultiTaskAgent.reward_multiplier = 10.

# Target network update rate
agent.Agent.tau = .004
agent.MultiTaskAgent.tau = .004

# Number of actions sampled for value estimation
agent.Agent.num_actions_for_value_in_critic_loss = 1
agent.MultiTaskAgent.num_actions_for_value_in_critic_loss = 3

agent.Agent.num_actions_for_value_in_actor_loss = 1
agent.MultiTaskAgent.num_actions_for_value_in_actor_loss = 3

# Offline RL coefficients for self-play
agent.Agent.online_coeff = 0.5
agent.MultiTaskAgent.online_coeff = 0.5
agent.Agent.offline_coeff = 0.5
agent.MultiTaskAgent.offline_coeff = 0.5
agent.Agent.fbc_filter_func = @agent.binary_filter
agent.MultiTaskAgent.fbc_filter_func = @agent.binary_filter

# =========================
# Core optimizer settings
# =========================
MetamonAMAGOExperiment.l2_coeff = 5e-5
MetamonAMAGOExperiment.learning_rate = 5e-5      # Base LR (conservative)
MetamonAMAGOExperiment.lr_warmup_steps = 200     # Shorter warmup for smaller run
MetamonAMAGOExperiment.max_lr = 1e-4             # Never above Ataraxos max
MetamonAMAGOExperiment.min_lr = 5e-6             # Matches paper floor

MetamonAMAGOExperiment.grad_clip = 0.3           # Tight clipping

MetamonAMAGOExperiment.critic_loss_weight = 10.

# =========================
# Dynamic damping (KL)
# =========================
MetamonAMAGOExperiment.use_dynamic_damping = True

# Target KL per policy update - CONSERVATIVE (matches Ataraxos)
MetamonAMAGOExperiment.target_kl_per_step = 0.01  # Lower than v2's 0.02
MetamonAMAGOExperiment.kl_tolerance = 2.0         # Wider tolerance: [0.005, 0.02]

# Start with moderate damping for 100k sample run
MetamonAMAGOExperiment.kl_coef_init = 0.08        # Slightly lighter than v2's 0.10
MetamonAMAGOExperiment.kl_coef_max = 0.20         # Same cap as v2

# Power-law schedule: SCALED FOR 100k SAMPLES
# With 50k schedule steps, coefficient decays meaningfully over the run
MetamonAMAGOExperiment.kl_power_alpha = 0.3       # Stronger than v2's 0.2
MetamonAMAGOExperiment.kl_schedule_steps = 50_000 # ~50% of data budget

# Adaptive controller (same sensitivity as v2)
MetamonAMAGOExperiment.lr_shrink_factor = 0.5     # Halve LR when KL too high
MetamonAMAGOExperiment.lr_grow_factor = 1.1       # Grow slowly when KL low

MetamonAMAGOExperiment.kl_coef_growth_factor = 2.0   # Ramp damping fast if KL high
MetamonAMAGOExperiment.kl_coef_decay_factor = 0.9    # Relax gently if KL low

# =========================
# Dynamic damping (entropy)
# =========================
MetamonAMAGOExperiment.ent_coef_init = 0.02       # Same as v2
MetamonAMAGOExperiment.ent_coef_min = 0.005       # Don't let exploration vanish

# Power-law entropy decay: SCALED FOR 100k SAMPLES
# Faster decay than KL (encourages shift from exploration to exploitation)
MetamonAMAGOExperiment.ent_power_alpha = 0.5      # Stronger than v2's 0.3
MetamonAMAGOExperiment.ent_schedule_steps = 50_000 # Match KL schedule

# =========================
# Key Metrics to Monitor
# =========================
# During training, watch these tensorboard plots:
#   1. damping/kl_coef - Should start ~0.08, decay gradually to ~0.04
#   2. damping/ent_coef - Should start 0.02, decay to ~0.010
#   3. KL Divergence - Should hover near 0.01, controller keeps it stable
#   4. damping/lr - Should adjust reactively (drops when KL spikes)
#   5. Policy Entropy - Should trend downward but stay above 1.0
#
# Red flags:
#   - KL consistently >> 0.02: Updates too aggressive, controller struggling
#   - Entropy < 0.5: Policy collapsed, needs stronger damping
#   - KL ~ 0.0: Controller too conservative, can increase target_kl
#   - LR keeps shrinking: Persistent instability, reduce kl_power_alpha
